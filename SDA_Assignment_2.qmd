---
title: "Home Assignment SDA: Part 2"
format: html
execute:
  echo: false
---

## Authors

| Name                       | Email                           | Personal number |
|:-----------------------|--------------------------|----------------------|
| Caroline Birkehammar       | caroline.birkehammar\@gmail.com | 19971118-5562   |
| Steven Hiram Rubio Vasquez | stevenrv97\@gmail.com           | 19970222-8793   |
| Pablo Paras Ochoa          | pablo.paras.ochoa\@gmail.com    | 19980810-0433   |
| Faezeh Karegar Hojatabadi  | faezeh.karegar\@gmail.com       | 19890903-1646   |


```{r,warning=FALSE,message=FALSE}
library(readxl)
library(tidyverse)
library(ggplot2)
library(cowplot)
library(broom)
data <- read_excel("dataset01.xlsx")
test <- read_excel("test.xlsx")

data$STARTING_PRICE <- data$STARTING_PRICE / 1000000
data[c("REGION", "TYPE", "BALCONY")] <- lapply(data[c("REGION", "TYPE", "BALCONY")], as.factor)
levels(data$BALCONY) <- c("0", "1")
```

# Task 1

***Using a significance level of 1%, test the hypothesis that half of the housing units in the region have a balcony.***

#### Data and problem description

```{r}
n_balconies <- sum(data$BALCONY == 1) # 390
n           <- length(data$BALCONY)   # 701
phat        <- n_balconies / n        # About 0.55635
p0          <- 0.5
se0         <- sqrt(p0 * (1 - p0) / n)
```

The observed proportion of balconies in our sample ($\hat{p}$) is equal to $0.55635$, but this is not enough to conclude whether or not half the housing units in the region have a balcony. What we want to know is actually how likely it is that we would observe $\hat{p} = 0.55635$ if the true value of $p$ is $0.5$. More formally, we first assume that $p = 0.5$ (which we denote our null hypothesis $H_{0}$) and estimate the likelihood of having observed $\hat{p} = 0.55635$ given that $H_{0}$ is true. A 1% significance level means that our test is designed so that, on average, we will incorrectly reject the null hypothesis 1% of the time. The null and alternative hypotheses are as follows: $$H_{0}: p = 0.5 \qquad H_{A}: p \ne 0.5$$ We can assume that the distribution of $\hat{p}$ is approximately normal (since sample estimators for samples of sufficient size are normally distributed around the real value). For the distribution, we use mean 0.5 ($H_{0}$) and standard deviation 0.019. Formally, the distribution is as follows: $$ Since \quad n\hat{p}\hat{q} = 175.25 > 5, \qquad \hat{p} \sim N\left(0.5, \sqrt{\frac{0.5 \cdot 0.5}{701}}\right) = N(0.5, 0.019) $$

#### Using the formula

```{r}
#| echo: true
z <- (phat - p0) / se0
p_value <- 2 * (1 - pnorm(abs(z)))
print(p_value)
```

#### Using built-in functions

```{r}
#| echo: true
test_balcony = prop.test(n_balconies, n = n, p = 0.5, alternative = "two.sided", conf.level = 0.99)
test_balcony$p.value
```


#### Conclusion

Since the p-value is less than 0.01 in both cases, the observed proportion of balconies is very unlikely under the null hypothesis, even on the 99% significance level. Therefore, we reject the null hypothesis that $p = 0.5$. However, it must be remarked that the proportion of balconies, while not exactly half, is still likely to somewhat above one half (though a one sided hypothesis test would be necesary to confirm that). 

# Task 2

***Using a significance level of 5%, test the hypothesis that the expected size of the housing units in the region is 75m2.***

#### Data and problem description

```{r}
alpha   <- 0.05
mu      <- 75
xbar    <- mean(data$AREA)
se      <- sqrt(sum((data$AREA - mu)^2) / (n-1)) # Population variance unknown
se_xbar <- se / sqrt(n) # Variance of xbar

data %>%
  ggplot() +
  geom_histogram(aes(AREA), fill = "steelblue", color = "grey15", binwidth = 4) + # Not really normally distributed...
  labs(title = "Distribution of area in our sample") +
  xlab("Area (in square meters)") +
  ylab("Absolute frequency in sample") +
  theme_classic()
```

The distribution of $x$ as shown by the histogram is right tailed and not normally distributed. However, because the sample size is greater than 30 we can approximate the distribution of $\bar{X}$ with a normal distribution using the central limit theorem.

The observed mean of the area in our sample (that is to say, the estimate) is equal to $79.2 m^2$ As in the previous problem, we can determine whether it is likely that the actual value is $75 m^2$ by estimating the likelihood that we would observe $\bar{x} = 79.2 m^2$ if $\bar{X} = 75 m^2$.

Therefore, our null and alternative hypotheses are as follows: $$H_{0}: \mu = 0.75 \qquad H_{A}: \mu \ne 0.75$$

#### Using the formula

```{r}
#| echo: true
t_obs2  <- (xbar - mu) / se_xbar # 54.47
p_value2 <- 2 * (1 - pt(q=t_obs2,df=n-1))
print(p_value2)
```

#### Using built-in functions

```{r}
#| echo: true
mean_test <- t.test(data$AREA, mu = mu, alternative = "two.sided", conf.level = 0.95)
mean_test$p.value
```


#### Conclusion

The p-value is either $0.0046$ or $0.0049$ depending on the method. Because the pre-established level of significance is 0.05 we can reject the null hypothesis. Our interpretation therefore, is that the mean (or expected) area of the housing units is not 75 square meters as it is highly unlikely we would have observed a value of $\bar{x} = 79.2 m^2$ if the actual value were $\bar{X} = 75 m^2$.

# Task 3

***Estimate the proportion of apartments in the region through a 90% confidence interval.***

#### Data and problem description

```{r}
#| echo: true
phat  <- sum(data$TYPE == "Apartment") / n
z     <- qnorm(0.95) # Two-sided, so 90% ci uses z = 0.95
se    <- sqrt(phat*(1-phat)/n)
```

Our sample's estimate for the proportion of apartments was $0.792$, but it is very unlikely that we have obtained exactly the same proportion of apartments in the sample as in the population. Therefore, we want to produce a confidence interval which we can be fairly sure includes the value in the population. More formally the 90% confidence interval for a proportion indicates that on average, the interval would contain the true value in 9 out of 10 samples. It can be expressed as follows: $$ \hat{p} \pm z_{0.05} \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} $$

#### Using the formula

```{r}
#| echo: true

conf_int <- c((phat - z*se), phat + z*se)
print(conf_int[2])

```

#### Using the built-in functions

```{r}
#| echo: true
ci <- prop.test(sum(data$TYPE == "Apartment"), n, conf.level = 0.9)
ci$conf.int[1:2]
```


#### Conclusion

The 90% confidence interval for the proportion of housing units that are apartments is about 0.76 to 0.82. This means that if we repeated this process with different samples an infinite number of times, 90% of calculated intervals would contain the real value.  

# Task 4

***Estimate the expected number of rooms of the housing units in the region through a 95% confidence interval.***

#### Data and problem description

```{r}
#| echo: true
xbar <- mean(data$ROOMS)  # About 3.15
se   <- sd(data$ROOMS)    # About 1.59
t975 <- qt(0.975, n-1)
```

This problem is very similar to the previous one. In this case, we want to create an interval that, if we obtained an infinite number of samples, would contain the actual value for the population 95% of the time. Additionally, since the variance is unknown, the distribution of the expected number of rooms will follow a t-distribution. Formally, the 95% confidence interval is defined as:

$$ \bar{x} \pm t_{0.025, n-1} \frac{s_x}{\sqrt{n}} $$

#### Using the formulas

```{r}
#| echo: true
conf_int <- c((xbar - t975 * se / sqrt(n)), xbar + t975 * se / sqrt(n))
print(conf_int)
```

#### Using the built-in functions

```{r}
#| echo: true
ci <- t.test(data$ROOMS, conf.level = 0.95)
ci$conf.int[1:2]
```

#### Conclusion

The confidence interval for the mean (expected) number of rooms in the region is \[3.033163, 3.269262\]. This means we can be fairly confident that the actual average number of rooms for housing regions is slightly above 3. It is important to note that even though a given unit of housing must have a discrete number of rooms, the expected number of rooms is not discrete.

# Task 5

***Fit a linear regression that explains STARTING_PRICE in terms of some or all of the remaining variables.***

#### Data exploration and transformation

Linear regressions enable us to make predictions for a variable given a different set of variables. In this case, we predict the Starting Price of the housing units as a function of Region, Type, Balcony, Rooms and Area as well as interactions and transformations among them. However, linear regression models make several assumptions that must be met when estimating a model using OLS. First, the relationship between the independent variables and the dependent variable must be linear. Second, the error terms must be independent. Third, the error terms should be homoskedastic, meaning that they have a constant variance independent of the values of the explanatory variables. Fourth and finally, the error terms should be normally distributed.

Our data have three categorical independent variables that are not subject to these assumptions: Region, Type and Balcony. The other two independent variables (Rooms and Area) as well as the dependent variable (Starting Price) are numerical, and we must check if the OLS assumptions hold for these. While the assumptions about the error terms are checked after fitting the model, the linearity assumption can be checked ex-ante. We do this by visualizing our data in scatterplots, evaluating if any transformation may be necessary to make the relationship between the dependent and independent variables linear.

We begin by examining the relationship between Rooms and Starting Price.
```{r}
par(mfrow = c(2, 2))
plot(STARTING_PRICE ~ ROOMS, data = data, main = "Without transformation")
plot(log(STARTING_PRICE) ~ ROOMS, data = data, main = "Log of Starting Price")
plot(STARTING_PRICE ~ log(ROOMS), data = data, main = "Log of Rooms")
plot(log(STARTING_PRICE) ~ log(ROOMS), data = data, main = "Log of Starting Price and Rooms")
```
The first plot shows an approximately linear relationship between Rooms and Starting Price, and there is no clear improvement by transforming the variables. As such, we considered it unnecessary to use any transformation. 

We follow this by checking the linearity assumption between Starting Price and Area.
```{r}
par(mfrow = c(2, 2))
plot(STARTING_PRICE ~ AREA, data = data, main = "Without transformation")
plot(log(STARTING_PRICE) ~ AREA, data = data, main = "Log of Starting Price")
plot(STARTING_PRICE ~ log(AREA), data = data, main = "Log of Area")
plot(log(STARTING_PRICE) ~ log(AREA), data = data, main = "Log of Starting Price and Area")

data <- read_excel("dataset01.xlsx")
data$STARTING_PRICE <- data$STARTING_PRICE / 1000000
data[c("REGION", "TYPE", "BALCONY")] <- lapply(data[c("REGION", "TYPE", "BALCONY")], as.factor)
```

Both the plot without any transformations and the plot where logs have been taken on both Starting Price and Area show a fairly linear relationship. Notably, the first plot is quite cone-shaped, indicating the presence of heteroskedasticity in the residuals. In contrast, the residuals in the plot of the transformed variables looks more even and homoskedastic.

From the plots of the numerical variables, we see that the linearity assumption holds for the unchanged data. However, as we observe that there seems to be less heteroskedasticity in the plot  using logs of Starting Price and Area, we will create an additional model using logs for these variables and evaluate if the OLS assumptions regarding residuals hold better for that model than the one without log transformations.

#### Fit multiple regression model

We create three multiple regression models: one without log transformations or interactions, one without any log transformations but with interactions, and one using both log transformations and interaction terms.

To choose which predictors to include in the fitting of each model, we use a stepwise backward selection algorithm. The algorithm starts with a model including all predictors, and then iteratively removes the least significant predictor by comparing the AIC (Akaike's Information Criterion) of the models, until there is no significant improvement to be made anymore. The choice of variable selection method we make in this task is fairly arbitrary; backwards stepwise selection is a widely used manner of selecting variables, but other methods (such as forward or pairwise selection, or completely different algorithms) would probably yield slightly different models. However, comparing different variable selection methods is beyond the scope of this report, and we settle for the stepwise backwards elimination algorithm here.

The first model (without transformations or interactions) is as follows:
```{r}
m1 <- lm(STARTING_PRICE ~ REGION + TYPE + BALCONY + ROOMS + AREA, data = data)

base_model <- step(m1, direction = "backward", trace = 0)
summary(base_model)
```

The second model (including interactions) is as follows:
```{r}

m2 <- lm(STARTING_PRICE ~ REGION + TYPE + BALCONY + ROOMS + AREA + 
          REGION*TYPE + REGION*BALCONY + REGION*ROOMS + REGION*AREA + 
          TYPE*BALCONY + TYPE*ROOMS + TYPE*AREA + 
          BALCONY*ROOMS + BALCONY*AREA + 
          ROOMS*AREA, data = data)

no_log_model <- step(m2, direction = "backward", trace = 0)
summary(no_log_model)
```

The third model (including interactions and logs of Starting Price and Area) is as follows:
```{r}
m3 <- lm(log(STARTING_PRICE) ~ REGION + TYPE + BALCONY + ROOMS + log(AREA) + 
          REGION*TYPE + REGION*BALCONY + REGION*ROOMS + REGION*log(AREA) + 
          TYPE*BALCONY + TYPE*ROOMS + TYPE*log(AREA) + 
          BALCONY*ROOMS + BALCONY*log(AREA) + 
          ROOMS*log(AREA), data = data)

log_model <- step(m3, direction = "backward", trace = 0)
summary(log_model)
```

Below, we examine the remaining OLS assumptions regarding the residuals for the three models.
```{r}
#Graphs
model1_aug <- augment(base_model)

obj1 <-
  model1_aug %>%
  ggplot(aes(.std.resid)) +
  geom_density() 

obj2 <-
  model1_aug %>% 
  ggplot(aes(x = .fitted, y = .std.resid))+
  geom_point(color = "cyan4") + 
  geom_hline(yintercept = 0, color = "salmon") +
  geom_smooth(se = F)

obj3 <-
  model1_aug %>% 
  ggplot(aes(sample = .std.resid)) +
  geom_qq_line(color = "salmon") +
  geom_qq(color = "cyan4") +
  labs(x = "Theoretical quantiles",
       y = "Observed quantiles")

plot_grid(obj1, obj2, obj3)

model2_aug <- augment(no_log_model)

obj1 <-
  model2_aug %>%
  ggplot(aes(.std.resid)) +
  geom_density() 

obj2 <-
  model2_aug %>% 
  ggplot(aes(x = .fitted, y = .std.resid))+
  geom_point(color = "cyan4") + 
  geom_hline(yintercept = 0, color = "salmon") +
  geom_smooth(se = F)

obj3 <-
  model2_aug %>% 
  ggplot(aes(sample = .std.resid)) +
  geom_qq_line(color = "salmon") +
  geom_qq(color = "cyan4") +
  labs(x = "Theoretical quantiles",
       y = "Observed quantiles")

plot_grid(obj1, obj2, obj3)

model3_aug <- augment(log_model)

obj1 <-
  model3_aug %>%
  ggplot(aes(.std.resid)) +
  geom_density() 

obj2 <-
  model3_aug %>% 
  ggplot(aes(x = .fitted, y = .std.resid))+
  geom_point(color = "cyan4") + 
  geom_hline(yintercept = 0, color = "salmon") +
  geom_smooth(se = F)

obj3 <-
  model3_aug %>% 
  ggplot(aes(sample = .std.resid)) +
  geom_qq_line(color = "salmon") +
  geom_qq(color = "cyan4") +
  labs(x = "Theoretical quantiles",
       y = "Observed quantiles")

plot_grid(obj1, obj2, obj3)

```


#### Conclusion and selected multiple regression model

We decided to select the third model for two key reasons. First and most importantly, as can be seen in the residual analysis, the first and second models do not comply with the model's assumptions. While in all three cases the distribution of the residuals is approximately normal, in the first two it is clear that there is a non-linear relationship between the theoretical and observed quantiles (and as such the error terms are likely not independent). Additionally, it is abundantly clear that the error terms do not have a constant standard deviation (the models are heteroskedastic).

The second and less important reason is that the third model explains more of the variance in our explained variable than the others (higher R squared). This improvement is not enormous (of about 0.06) and comes at the cost of a far more complex model which makes interpretation of our results considerably more difficult. However, we considered it worthwhile to make a model with slightly more predictive capabilities and, more importantly, to make a model that fits our model assumptions.

The best model is written out below as an equation. The reference group is apartments in region Northeast without a balcony. This model includes all variables that are significant on a 10% level, and interactions are marked by a colon symbol.
$$
\begin{align}
  \text{log}(\mathit{Starting\ Price}) &= -1.938 \\
  &\quad + 3.003 \times RegionSoutheast -0.143 \times RegionStockholm + 0.923 \times RegionWest\\
  &\quad -0.079 \times TypeTerrace + 0.266 \times TypeVilla\\
  &\quad -0.057 \times Balcony \\
  &\quad +0.061 \times Rooms\\
  &\quad +0.686 \times \text{log}(\mathit{Area})\\
  &\quad -0.422 \times RegionStockholm:TypeTerrace -0.315 \times RegionStockholm:TypeVilla\\
  &\quad +0.188 \times RegionSoutheast:Rooms\\
  &\quad -0.902 \times RegionSoutheast:\text{log}(\mathit{Area}) -0.386 \times RegionWest:\text{log}(\mathit{Area})\\
\end{align}
$$

# Task 6

***Use your regression model from exercise 5 to predict the starting price of these housing units***

#### Point and Interval prediction

```{r}
pred1 <- predict(base_model,test,interval = "prediction", level = 0.90)
pred2 <-predict(no_log_model,test,interval = "prediction", level = 0.90)
pred3 <-exp(predict(log_model,test,interval = "prediction", level = 0.90))

```
These are the predictions $\hat{y}$ for each new observation using our multiple regression model.
```{r}
pred3[,1]
```

These are the prediction intervals for the observations.
```{r}
pred3[,2:3]
```

#### Conclusion

Pablo writes

#### Extra

```{r}

model1Frame <- data.frame(Variable = rownames(pred1),
                          Coefficient = pred1[,1],
                          SE=abs(pred1[,1]-pred1[,3]),
                          modelName = "No interaction or logs")
model2Frame <- data.frame(Variable = rownames(pred2),
                          Coefficient = pred2[,1],
                          SE=abs(pred2[,1]-pred2[,3]),
                          modelName = "With interactions")
model3Frame <- data.frame(Variable = rownames(pred3),
                          Coefficient = pred3[,1],
                          SE=abs(pred3[,1]-pred3[,3]),
                          modelName = "With interactions and logs")
# Combine these data.frames
allModelFrame <- data.frame(rbind(model1Frame, model2Frame, model3Frame))  # etc.

# Plot
zp1 <- ggplot(allModelFrame, aes(colour = modelName))
zp1 <- zp1 + geom_hline(yintercept = 0, colour = gray(1/2), lty = 2)
zp1 <- zp1 + geom_linerange(aes(x = Variable, ymin = Coefficient - SE,
                                ymax = Coefficient + SE),
                            lwd = 1, position = position_dodge(width = 1/2))
zp1 <- zp1 + geom_pointrange(aes(x = Variable, y = Coefficient, ymin = Coefficient - SE,
                                 ymax = Coefficient + SE),
                             lwd = 1/2, position = position_dodge(width = 1/2),
                             shape = 21, fill = "WHITE")
zp1 <- zp1 + coord_flip() + theme_bw()
zp1 <- zp1 + ggtitle("Comparing all models")+ylab("Predicted Starting price") +xlab("House predicted") 
print(zp1)  
```


